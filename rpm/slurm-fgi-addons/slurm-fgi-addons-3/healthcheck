#!/bin/bash

# SLURM health check program
# ulf.tigerstedt@csc.fi 2012

FAILED=0
ERROR=""
HOSTNAME=`hostname -s`
DEBUG=""

STATELINE=`scontrol -o show node $HOSTNAME`
# Check if this is a SLURM worker node at all
if [ $? = 1 ] ; then
	#echo Not a slurm node
	exit
fi
if [ "$1" = "-d" ]; then
	DEBUG="1"
fi

# Mangle the scontrol output into $LABEL=$PARAMETER values
# Available parameters:
# NodeName=ae5 Arch=x86_64 CoresPerSocket=6 CPUAlloc=0 CPUErr=0 CPUTot=12 Features=(null) Gres=(null) OS=Linux RealMemory=23000 Sockets=2 State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=1 BootTime=2012-01-31T17:10:45 SlurmdStartTime=2012-01-31T17:11:23 Reason=(null)

for a in $STATELINE; do
	LABEL=`echo $a | cut -d = -f 1`
	PARAMETER=`echo $a | cut -d = -f 2`
	
	if [ $LABEL = "Reason" ]; then 
		REASON=$PARAMETER
	fi
	if [ $LABEL = "State" ]; then 
		STATE=$PARAMETER
	fi
done
if [ -n "$DEBUG" ]; then echo Slurm thinks $HOSTNAME has STATE=$STATE and REASON=$REASON; fi


if [ "$REASON" = "rebooting" ]; then
	if [ "$STATE" = "DOWN+DRAIN" -o "$STATE" = "IDLE+DRAIN" ]; then
		if [ -n "$DEBUG" ]; then echo Resuming after reboot ; fi
		scontrol update NodeName=$HOSTNAME state=RESUME
	fi
fi


if [ "$REASON" = "reboot" -a "$STATE" = "IDLE+DRAIN" ]; then
	if [ -n "$DEBUG" ]; then echo Rebooting ; fi
	scontrol update NodeName=$HOSTNAME state=DOWN reason=rebooting
	sleep 2
	# stop slurm just in case
	service slurm stop
	/sbin/reboot 
fi

if [ -n "$DEBUG" ]; then echo Starting health check; fi
# Check if /home is readable by using a helper program


if [ -n "$DEBUG" ]; then echo NFS /home test starts; fi
if ps -C healthcheck-nfs > /dev/null; then 
	FAILED=$((FAILED+1)) 
	ERROR="NFSHOME"
else 
	/usr/bin/healthcheck-nfs /home & 
	sleep 40 
	if ps -C healthcheck-nfs > /dev/null ; then
		FAILED=$((FAILED+1))
		ERROR="NFSHOME"
	fi
fi 
if [ -n "$DEBUG" ]; then echo NFS /home test ends; fi
# Check if /tmp has over 200 megabytes free

if [ -n "$DEBUG" ]; then echo Testing disk free on /tmp; fi
if /usr/bin/healthcheck-df.pl; then 
	#echo DF is ok
	/bin/true
else 
	FAILED=$((FAILED+1))
	ERROR="$ERROR DF"
fi

# If there is supposed to be CVMFS mounted, check it.
if [ -n "$DEBUG" ]; then echo Testing CVMFS; fi
if [ -d /etc/cvmfs ]; then
	if [ ! -f /cvmfs/fgi.csc.fi/tools/file_for_testing ]; then
		FAILED=$((FAILED+1))
		ERROR="$ERROR CVMFS"
	fi
fi

if [ -n "$DEBUG" ]; then echo Starting local health checks; fi
if [ -d /etc/slurm/healthcheck ]; then 
	for a in /etc/slurm/healthcheck/*.sh; do 
		source $a
	done 
fi

if [ -n "$DEBUG" ]; then 
	echo Health checks done: $FAILED fails.	
	echo Reason: $ERROR
fi

if [ $FAILED -gt 0 ]; then 
	if [ -n "$DEBUG" ]; then echo Draining node; fi
	scontrol update NodeName=$HOSTNAME state=DRAIN reason="healthcheck_failed $ERROR"
fi
# Return a node to service if it has recovered

if [ $FAILED -eq 0 ]; then 
       if [ "$STATE" = "DOWN+DRAIN" -o "$STATE" = "IDLE+DRAIN" ]; then
		if [[ "$REASON"  =~  ^healthcheck_failed.* ]]; then 
			if [ -n "$DEBUG" ]; then echo Node was drained but is now returning to operations ; fi
			scontrol update NodeName=$HOSTNAME state=RESUME 
		fi
	fi
fi


if [ -n "$DEBUG" ]; then echo Health check done; fi

